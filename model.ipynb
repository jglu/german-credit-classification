{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# STAT 441: Final Project\n",
    "\n",
    "Author: Jessica Lu\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data\n",
    "\n",
    "This dataset comes courtesy from the UC Irvine Machine Learning Repository.\n",
    "\n",
    "---\n",
    "\n",
    "Models:\n",
    "- Logistic regression\n",
    "- kNN\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "We download the dataset from the internet. The dataset comes in a both a numeric-only and numeric+categorical format. The data file is in an unconventional format, so we convert both to a .csv. We add an id column to the .csv file.\n",
    "\n",
    "Instead of getting the dataset like this:\n",
    "\n",
    "```\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "statlog_german_credit_data = fetch_ucirepo(id=144) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = statlog_german_credit_data.data.features \n",
    "y = statlog_german_credit_data.data.targets \n",
    "```\n",
    "\n",
    "I have downloaded the data file from the website and wrote to a csv file. I chose meaningful feature names, as opposed to \"Attribute#\". I also added an id column. The dataset is assumed to be located relatively at `./datasets/german.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Pre-preprocessing (shared code for all models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/german.csv\")\n",
    "\n",
    "# map credit_risk to 0/1 (good/bad)\n",
    "y = df[\"credit_risk\"].map({1: 0, 2: 1})\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "X = df.drop(columns=[\"credit_risk\", \"id\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for data imbalance? \n",
    "print(y_train.value_counts())   # 560/240\n",
    "print(y_test.value_counts())    # 140/60\n",
    "\n",
    "# we find that our data is not imbalanced so we can use accuracy as a metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Shared code for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0] # 1000 instances\n",
    "\n",
    "# used in transforming the data with ColumnTransformer()\n",
    "numeric_features = X.select_dtypes(include=\"number\").columns\n",
    "categorical_features = X.select_dtypes(exclude=\"number\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## output best params found in hyperparameter tuning\n",
    "# use cost matrix provided from https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data\n",
    "def cost_score(y_true, y_pred):\n",
    "    cost_matrix = np.array([[0, 1],\n",
    "                        [5, 0]])\n",
    "    total_cost = sum(cost_matrix[t-1, p-1] for t, p in zip(y_true, y_pred))\n",
    "    return total_cost\n",
    "\n",
    "# using same scoring metrics across all four models\n",
    "metrics = [\"cost\", \"accuracy\", \"f1\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"f1\": make_scorer(f1_score, zero_division=0),\n",
    "    \"precision\": make_scorer(precision_score, zero_division=0),\n",
    "    \"recall\": make_scorer(recall_score, zero_division=0),\n",
    "    \"roc_auc\": \"roc_auc\",\n",
    "    \"cost\": make_scorer(cost_score, greater_is_better=False)\n",
    "}\n",
    "\n",
    "# wrapper function to print best parameter set for each metric.\n",
    "#   called after training\n",
    "def print_best_params_by_metric(cv_results, metrics, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Outputs best parameter set for each metric.\n",
    "    Corrects sign for the 'cost' metric, because greater_is_better=False\n",
    "    means scikit-learn internally stores negative costs.\n",
    "    \"\"\"\n",
    "    print(f\"For {model_name}:\")\n",
    "    for metric in metrics:\n",
    "        best_index = cv_results[f\"mean_test_{metric}\"].argmax()\n",
    "\n",
    "        best_params = {\n",
    "            k.replace(\"param_model__\", \"\"): v[best_index]\n",
    "            for k, v in cv_results.items()\n",
    "            if k.startswith(\"param_model__\")\n",
    "        }\n",
    "\n",
    "        # raw CV score (for cost, this is negative)\n",
    "        best_score = cv_results[f\"mean_test_{metric}\"][best_index]\n",
    "        \n",
    "        # flip sign for cost\n",
    "        if metric == \"cost\":\n",
    "            corrected = -best_score\n",
    "            print(f\"Best params for {metric}: {best_params}\")\n",
    "            print(f\"Best CV {metric}: {corrected}  (corrected from {best_score})\")\n",
    "        else:\n",
    "            print(f\"Best params for {metric}: {best_params}\")\n",
    "            print(f\"Best CV {metric}: {best_score}\")\n",
    "        print()\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## output test metrics for best fit model\n",
    "def print_test_metrics(model, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Prints test set metrics for a fitted model in a simple key-value style.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: fitted sklearn pipeline or classifier\n",
    "    - model_name: string for labeling output\n",
    "    \"\"\"\n",
    "    best_params = model.best_params_\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_prob),\n",
    "        \"cost\": cost_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"Test {metric}: {value}\")\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "np.logspace(-4,4,20) = (array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n",
    "        4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
    "        2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
    "        1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
    "        5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "# thanks to: https://www.youtube.com/watch?v=tIO8zPCdi58\n",
    "# thanks to: https://www.geeksforgeeks.org/machine-learning/how-to-optimize-logistic-regression-performance/\n",
    "\n",
    "preprocess_lr = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    (\"preprocess\", preprocess_lr),\n",
    "    (\"model\", LogisticRegression(class_weight=\"balanced\",\n",
    "                                 random_state=SEED))\n",
    "])\n",
    "\n",
    "# \"The main hyperparameters we may tune in logistic regression are: solver, penalty, and regularization strength (sklearn documentation).\"\n",
    "#   from: https://medium.com/codex/do-i-need-to-tune-logistic-regression-hyperparameters-1cb2b81fca69\n",
    "# \"Logistic regression does not really have any critical hyperparameters to tune.\"\n",
    "#   from: https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "\n",
    "param_grid_lr = [\n",
    "    {'model__penalty': ['l1'], 'model__solver': ['liblinear', 'saga'], 'model__C': np.logspace(-4, 4, 20), \"model__max_iter\": [100,1000,2500,5000]},\n",
    "    {'model__penalty': ['l2'], 'model__solver': ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga'], 'model__C': np.logspace(-4, 4, 20), \"model__max_iter\": [100,1000,2500,5000]},\n",
    "    {'model__penalty': ['elasticnet'], 'model__solver': ['saga'], 'model__C': np.logspace(-4, 4, 20), 'model__l1_ratio': [0.25, 0.5, 0.75], \"model__max_iter\": [100,1000,2500,5000]}\n",
    "]\n",
    "\n",
    "grid_lr = GridSearchCV(\n",
    "    estimator=pipeline_lr,  \n",
    "    param_grid=param_grid_lr, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"cost\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "# Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n",
    "# ran for like 4m..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "# thanks to: https://medium.com/@agrawalsam1997/hyperparameter-tuning-of-knn-classifier-a32f31af25c7\n",
    "#   for this: np.arange(2, 30, 1)\n",
    "\n",
    "# i purposely don't drop=\"first\"\n",
    "preprocess_knn = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_knn = Pipeline([\n",
    "    (\"preprocess\", preprocess_knn),\n",
    "    (\"model\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# notice that n=1000 and sqrt(1000) ~= 31\n",
    "param_grid_knn = {\n",
    "    'model__n_neighbors': np.arange(2, 30, 1), \n",
    "    \"model__weights\": [\"uniform\", \"distance\"],\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(\n",
    "    estimator=pipeline_knn,  \n",
    "    param_grid=param_grid_knn, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"cost\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "# ran for 14.1s very fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params for accuracy: {'n_neighbors': np.int64(10), 'weights': 'distance'}\n",
    "# Best CV accuracy: 0.7425\n",
    "# ------\n",
    "# Best params for f1: {'n_neighbors': np.int64(3), 'weights': 'uniform'}\n",
    "# Best CV f1: 0.45962787039846775\n",
    "# ------\n",
    "# Best params for precision: {'n_neighbors': np.int64(25), 'weights': 'distance'}\n",
    "# Best CV precision: 0.8350000000000002\n",
    "# ------\n",
    "# Best params for recall: {'n_neighbors': np.int64(2), 'weights': 'distance'}\n",
    "# Best CV recall: 0.39583333333333337\n",
    "# ------\n",
    "# Best params for roc_auc: {'n_neighbors': np.int64(24), 'weights': 'distance'}\n",
    "# Best CV roc_auc: 0.75859375\n",
    "# ------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "# thanks to: https://medium.com/@kalpit.sharma/mastering-random-forest-hyperparameter-tuning-for-enhanced-machine-learning-models-2d1a8c6c426f\n",
    "\n",
    "# no need to scale numerical features\n",
    "preprocess_rf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    (\"preprocess\", preprocess_rf),\n",
    "    (\"model\", RandomForestClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "# notice that n=1000 and sqrt(1000) ~= 31, there are 20 features.\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [50, 100, 150],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__max_depth': [None, 10, 20, 30],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=pipeline_rf,  \n",
    "    param_grid=param_grid_rf, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"cost\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
    "# ran for 11m 49.6s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Docs:\n",
    "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "# thanks to: https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning\n",
    "#       https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html\n",
    "#       https://xgboost.readthedocs.io/en/stable/parameter.html \n",
    "#       https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/#h-learning-task-parameters\n",
    "\n",
    "# no need to scale numerical features\n",
    "preprocess_xgb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_xgb = Pipeline([\n",
    "    (\"preprocess\", preprocess_xgb),\n",
    "    (\"model\", XGBClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'model__n_estimators': [50, 100, 150],\n",
    "    'model__gamma': [0, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__min_child_weight': [1, 3, 5],\n",
    "    'model__subsample': [0.7, 0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    estimator=pipeline_xgb,  \n",
    "    param_grid=param_grid_xgb, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"cost\",\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
    "# ran for 6m 6.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params for accuracy: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.2), 'max_depth': np.int64(7), 'min_child_weight': np.int64(3), 'n_estimators': np.int64(50), 'subsample': np.float64(0.7)}\n",
    "# Best CV accuracy: 0.7700000000000001\n",
    "# ------\n",
    "# Best params for f1: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.2), 'max_depth': np.int64(7), 'min_child_weight': np.int64(3), 'n_estimators': np.int64(50), 'subsample': np.float64(0.7)}\n",
    "# Best CV f1: 0.573550385273438\n",
    "# ------\n",
    "# Best params for precision: {'colsample_bytree': np.float64(0.7), 'gamma': np.float64(0.1), 'max_depth': np.int64(3), 'min_child_weight': np.int64(3), 'n_estimators': np.int64(50), 'subsample': np.float64(1.0)}\n",
    "# Best CV precision: 0.6433135949248482\n",
    "# ------\n",
    "# Best params for recall: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.2), 'max_depth': np.int64(5), 'min_child_weight': np.int64(3), 'n_estimators': np.int64(100), 'subsample': np.float64(0.8)}\n",
    "# Best CV recall: 0.5333333333333333\n",
    "# ------\n",
    "# Best params for roc_auc: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.1), 'max_depth': np.int64(3), 'min_child_weight': np.int64(1), 'n_estimators': np.int64(50), 'subsample': np.float64(0.7)}\n",
    "# Best CV roc_auc: 0.7905133928571428\n",
    "# ------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "models = {\n",
    "    \"logistic_regression\": grid_lr,\n",
    "    \"knn\": grid_knn,\n",
    "    \"random_forest\": grid_rf,\n",
    "    \"xgboost\": grid_xgb\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    path = f\"models/{name}.pkl\"\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"Saved {name} to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## output best parameters for each model\n",
    "print_best_params_by_metric(grid_lr.cv_results_, metrics, \"Logistic Regression\")\n",
    "print_best_params_by_metric(grid_knn.cv_results_, metrics, \"kNN\")\n",
    "print_best_params_by_metric(grid_rf.cv_results_, metrics, \"Random Forest\")\n",
    "print_best_params_by_metric(grid_xgb.cv_results_, metrics, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Logistic Regression:\n",
    "# Best params for cost: {'C': np.float64(0.00026366508987303583), 'max_iter': np.int64(100), 'penalty': 'l2', 'solver': 'liblinear', 'l1_ratio': masked}\n",
    "# Best CV cost: 47.8  (corrected from -47.8)\n",
    "\n",
    "# Best params for accuracy: {'C': np.float64(0.615848211066026), 'max_iter': np.int64(100), 'penalty': 'l2', 'solver': 'lbfgs', 'l1_ratio': masked}\n",
    "# Best CV accuracy: 0.75125\n",
    "\n",
    "# Best params for f1: {'C': np.float64(4.281332398719396), 'max_iter': np.int64(1000), 'penalty': 'elasticnet', 'solver': 'saga', 'l1_ratio': np.float64(0.75)}\n",
    "# Best CV f1: 0.5278343535463698\n",
    "\n",
    "# Best params for precision: {'C': np.float64(0.004832930238571752), 'max_iter': np.int64(100), 'penalty': 'l2', 'solver': 'liblinear', 'l1_ratio': masked}\n",
    "# Best CV precision: 0.7171428571428571\n",
    "\n",
    "# Best params for recall: {'C': np.float64(78.47599703514607), 'max_iter': np.int64(100), 'penalty': 'l2', 'solver': 'newton-cg', 'l1_ratio': masked}\n",
    "# Best CV recall: 0.4791666666666667\n",
    "\n",
    "# Best params for roc_auc: {'C': np.float64(0.615848211066026), 'max_iter': np.int64(100), 'penalty': 'l2', 'solver': 'sag', 'l1_ratio': masked}\n",
    "# Best CV roc_auc: 0.7853050595238096\n",
    "\n",
    "# ------\n",
    "# For kNN:\n",
    "# Best params for cost: {'n_neighbors': np.int64(25), 'weights': 'distance'}\n",
    "# Best CV cost: 47.8  (corrected from -47.8)\n",
    "\n",
    "# Best params for accuracy: {'n_neighbors': np.int64(10), 'weights': 'distance'}\n",
    "# Best CV accuracy: 0.7425\n",
    "\n",
    "# Best params for f1: {'n_neighbors': np.int64(3), 'weights': 'uniform'}\n",
    "# Best CV f1: 0.45962787039846775\n",
    "\n",
    "# Best params for precision: {'n_neighbors': np.int64(25), 'weights': 'distance'}\n",
    "# Best CV precision: 0.8350000000000002\n",
    "\n",
    "# Best params for recall: {'n_neighbors': np.int64(2), 'weights': 'distance'}\n",
    "# Best CV recall: 0.39583333333333337\n",
    "\n",
    "# Best params for roc_auc: {'n_neighbors': np.int64(24), 'weights': 'distance'}\n",
    "# Best CV roc_auc: 0.75859375\n",
    "\n",
    "# ------\n",
    "# For Random Forest:\n",
    "# Best params for cost: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': np.int64(1), 'min_samples_split': np.int64(5), 'n_estimators': np.int64(100)}\n",
    "# Best CV cost: 55.2  (corrected from -55.2)\n",
    "\n",
    "# Best params for accuracy: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': np.int64(1), 'min_samples_split': np.int64(5), 'n_estimators': np.int64(50)}\n",
    "# Best CV accuracy: 0.7775000000000001\n",
    "\n",
    "# Best params for f1: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': np.int64(1), 'min_samples_split': np.int64(5), 'n_estimators': np.int64(50)}\n",
    "# Best CV f1: 0.5223626341081272\n",
    "\n",
    "# Best params for precision: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': np.int64(1), 'min_samples_split': np.int64(5), 'n_estimators': np.int64(100)}\n",
    "# Best CV precision: 0.8001082251082252\n",
    "\n",
    "# Best params for recall: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': np.int64(1), 'min_samples_split': np.int64(5), 'n_estimators': np.int64(50)}\n",
    "# Best CV recall: 0.4083333333333334\n",
    "\n",
    "# Best params for roc_auc: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': np.int64(2), 'min_samples_split': np.int64(10), 'n_estimators': np.int64(150)}\n",
    "# Best CV roc_auc: 0.7994419642857143\n",
    "\n",
    "# ------\n",
    "# For XGBoost:\n",
    "# Best params for cost: {'colsample_bytree': np.float64(0.7), 'gamma': np.float64(0.0), 'max_depth': np.int64(3), 'min_child_weight': np.int64(1), 'n_estimators': np.int64(50), 'subsample': np.float64(0.7)}\n",
    "# Best CV cost: 90.4  (corrected from -90.4)\n",
    "\n",
    "# Best params for accuracy: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.2), 'max_depth': np.int64(7), 'min_child_weight': np.int64(3), 'n_estimators': np.int64(50), 'subsample': np.float64(0.7)}\n",
    "# Best CV accuracy: 0.7700000000000001\n",
    "\n",
    "# Best params for f1: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.2), 'max_depth': np.int64(7), 'min_child_weight': np.int64(3), 'n_estimators': np.int64(50), 'subsample': np.float64(0.7)}\n",
    "# Best CV f1: 0.573550385273438\n",
    "\n",
    "# Best params for precision: {'colsample_bytree': np.float64(0.7), 'gamma': np.float64(0.1), 'max_depth': np.int64(3), 'min_child_weight': np.int64(3), 'n_estimators': np.int64(50), 'subsample': np.float64(1.0)}\n",
    "# Best CV precision: 0.6433135949248482\n",
    "\n",
    "# Best params for recall: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.2), 'max_depth': np.int64(5), 'min_child_weight': np.int64(3), 'n_estimators': np.int64(100), 'subsample': np.float64(0.8)}\n",
    "# Best CV recall: 0.5333333333333333\n",
    "\n",
    "# Best params for roc_auc: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.1), 'max_depth': np.int64(3), 'min_child_weight': np.int64(1), 'n_estimators': np.int64(50), 'subsample': np.float64(0.7)}\n",
    "# Best CV roc_auc: 0.7905133928571428\n",
    "\n",
    "# ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit each model with cost and\n",
    "# output each model with test metrics\n",
    "print_test_metrics(grid_lr, \"Logistic Regression\")\n",
    "print_test_metrics(grid_knn, \"kNN\")\n",
    "print_test_metrics(grid_rf, \"Random Forest\")\n",
    "print_test_metrics(grid_xgb, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with cost\n",
    "\n",
    "# Best parameters for Logistic Regression: {'model__C': np.float64(0.00026366508987303583), 'model__max_iter': 100, 'model__penalty': 'l2', 'model__solver': 'liblinear'}\n",
    "# Test accuracy: 0.7\n",
    "# Test f1: 0.0\n",
    "# Test precision: 0.0\n",
    "# Test recall: 0.0\n",
    "# Test roc_auc: 0.7448809523809523\n",
    "# Test cost: 60\n",
    "# ------\n",
    "# Best parameters for kNN: {'model__n_neighbors': np.int64(25), 'model__weights': 'distance'}\n",
    "# Test accuracy: 0.76\n",
    "# Test f1: 0.4146341463414634\n",
    "# Test precision: 0.7727272727272727\n",
    "# Test recall: 0.2833333333333333\n",
    "# Test roc_auc: 0.7644047619047618\n",
    "# Test cost: 68\n",
    "# ------\n",
    "# Best parameters for Random Forest: {'model__criterion': 'gini', 'model__max_depth': 10, 'model__max_features': 'log2', 'model__min_samples_leaf': 1, 'model__min_samples_split': 5, 'model__n_estimators': 100}\n",
    "# Test accuracy: 0.76\n",
    "# Test f1: 0.4666666666666667\n",
    "# Test precision: 0.7\n",
    "# Test recall: 0.35\n",
    "# Test roc_auc: 0.8003571428571428\n",
    "# Test cost: 84\n",
    "# ------\n",
    "# Best parameters for XGBoost: {'model__colsample_bytree': 0.7, 'model__gamma': 0, 'model__max_depth': 3, 'model__min_child_weight': 1, 'model__n_estimators': 50, 'model__subsample': 0.7}\n",
    "# Test accuracy: 0.765\n",
    "# Test f1: 0.5765765765765766\n",
    "# Test precision: 0.6274509803921569\n",
    "# Test recall: 0.5333333333333333\n",
    "# Test roc_auc: 0.7971428571428572\n",
    "# Test cost: 123\n",
    "# ------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
