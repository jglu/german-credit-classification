{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938b57a6",
   "metadata": {},
   "source": [
    "# STAT 441: Final Project\n",
    "\n",
    "Author: Jessica Lu\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data\n",
    "\n",
    "This dataset comes courtesy from the UC Irvine Machine Learning Repository.\n",
    "\n",
    "---\n",
    "\n",
    "Models:\n",
    "- Logistic regression\n",
    "- kNN\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "We download the dataset from the internet. The dataset comes in a both a numeric-only and numeric+categorical format. The data file is in an unconventional format, so we convert both to a .csv. We add an id column to the .csv file.\n",
    "\n",
    "Instead of getting the dataset like this:\n",
    "\n",
    "```\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "statlog_german_credit_data = fetch_ucirepo(id=144) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = statlog_german_credit_data.data.features \n",
    "y = statlog_german_credit_data.data.targets \n",
    "```\n",
    "\n",
    "I have downloaded the data file from the website and wrote to a csv file. I chose meaningful feature names, as opposed to \"Attribute#\". I also added an id column. The dataset is assumed to be located relatively at `./datasets/german.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e27094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f918e3",
   "metadata": {},
   "source": [
    "## Pre-preprocessing (shared code for all models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85828d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/german.csv\")\n",
    "\n",
    "# map credit_risk to 0/1 (good/bad)\n",
    "y = df[\"credit_risk\"].map({1: 0, 2: 1})\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "X = df.drop(columns=[\"credit_risk\", \"id\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c266681",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tmp: eda\n",
    "\n",
    "y_train.value_counts() # 560/240\n",
    "y_test.value_counts() #140/60\n",
    "\n",
    "# not imbalanced so we can use accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ac885",
   "metadata": {},
   "source": [
    "## Shared code for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4112a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0] # 1000 instances\n",
    "\n",
    "\n",
    "# used in transforming the data with ColumnTransformer()\n",
    "numeric_features = X.select_dtypes(include=\"number\").columns\n",
    "categorical_features = X.select_dtypes(exclude=\"number\").columns\n",
    "\n",
    "\n",
    "# using same scoring metrics across all four models\n",
    "metrics = [\"accuracy\", \"f1\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"f1\": make_scorer(f1_score, zero_division=0),\n",
    "    \"precision\": make_scorer(precision_score, zero_division=0),\n",
    "    \"recall\": make_scorer(recall_score, zero_division=0),\n",
    "    \"roc_auc\": \"roc_auc\"\n",
    "}\n",
    "\n",
    "\n",
    "# wrapper function to print  best parameter set for each metric.\n",
    "#   called after training\n",
    "def print_best_params_by_metric(cv_results, metrics):\n",
    "    \"\"\"\n",
    "    Outputs best parameter set for each metric.\n",
    "    \"\"\"\n",
    "    for metric in metrics:\n",
    "        best_index = cv_results[f\"mean_test_{metric}\"].argmax()\n",
    "        best_params = {k.replace(\"param_model__\", \"\"): v[best_index] \n",
    "                       for k, v in cv_results.items()\n",
    "                        if k.startswith(\"param_model__\")}\n",
    "        best_score = cv_results[f\"mean_test_{metric}\"][best_index]\n",
    "        print(f\"Best params for {metric}: {best_params}\")\n",
    "        print(f\"Best CV {metric}: {best_score}\")\n",
    "        print(\"------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecf610",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "np.logspace(-4,4,20) = (array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n",
    "        4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
    "        2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
    "        1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
    "        5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "# thanks to: https://www.youtube.com/watch?v=tIO8zPCdi58\n",
    "# thanks to: https://www.geeksforgeeks.org/machine-learning/how-to-optimize-logistic-regression-performance/\n",
    "\n",
    "preprocess_lr = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    (\"preprocess\", preprocess_lr),\n",
    "    (\"model\", LogisticRegression(random_state=SEED))\n",
    "])\n",
    "\n",
    "# \"The main hyperparameters we may tune in logistic regression are: solver, penalty, and regularization strength (sklearn documentation).\"\n",
    "#   from: https://medium.com/codex/do-i-need-to-tune-logistic-regression-hyperparameters-1cb2b81fca69\n",
    "# \"Logistic regression does not really have any critical hyperparameters to tune.\"\n",
    "#   from: https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "param_grid_lr = {\n",
    "    # 'model__penalty': [\"l1\", \"l2\", \"elasticnet\"], # got thrown issue for this so\n",
    "    'model__C': np.logspace(-4,4,20),\n",
    "    \"model__solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"],\n",
    "    \"model__max_iter\": [100,1000,2500,5000]\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(\n",
    "    estimator=pipeline_lr,  \n",
    "    param_grid=param_grid_lr, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "# Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n",
    "# ran for like 4m..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all metrics, print best parameters\n",
    "cv_results_lr = grid_lr.cv_results_\n",
    "print_best_params_by_metric(cv_results_lr, metrics)\n",
    "\n",
    "# Best params for accuracy: {'C': np.float64(0.615848211066026), 'max_iter': np.int64(100), 'solver': 'lbfgs'}\n",
    "# Best CV accuracy: 0.75125\n",
    "# ------\n",
    "# Best params for f1: {'C': np.float64(10000.0), 'max_iter': np.int64(100), 'solver': 'sag'}\n",
    "# Best CV f1: 0.5252145534249706\n",
    "# ------\n",
    "# Best params for precision: {'C': np.float64(0.004832930238571752), 'max_iter': np.int64(100), 'solver': 'liblinear'}\n",
    "# Best CV precision: 0.7171428571428571\n",
    "# ------\n",
    "# Best params for recall: {'C': np.float64(78.47599703514607), 'max_iter': np.int64(100), 'solver': 'newton-cg'}\n",
    "# Best CV recall: 0.4791666666666667\n",
    "# ------\n",
    "# Best params for roc_auc: {'C': np.float64(0.615848211066026), 'max_iter': np.int64(100), 'solver': 'newton-cholesky'}\n",
    "# Best CV roc_auc: 0.7853050595238096\n",
    "# ------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a641952",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8daecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "# thanks to: https://medium.com/@agrawalsam1997/hyperparameter-tuning-of-knn-classifier-a32f31af25c7\n",
    "#   for this: np.arange(2, 30, 1)\n",
    "\n",
    "# i purposely don't drop=\"first\"\n",
    "preprocess_knn = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_knn = Pipeline([\n",
    "    (\"preprocess\", preprocess_knn),\n",
    "    (\"model\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# notice that n=1000 and sqrt(1000) ~= 31\n",
    "param_grid_knn = {\n",
    "    'model__n_neighbors': np.arange(2, 30, 1), \n",
    "    \"model__weights\": [\"uniform\", \"distance\"],\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(\n",
    "    estimator=pipeline_knn,  \n",
    "    param_grid=param_grid_knn, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "# ran for 14.1s very fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25589fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all metrics, print best parameters\n",
    "cv_results_knn = grid_knn.cv_results_\n",
    "print_best_params_by_metric(cv_results_knn, metrics)\n",
    "\n",
    "# Best params for accuracy: {'n_neighbors': np.int64(10), 'weights': 'distance'}\n",
    "# Best CV accuracy: 0.7425\n",
    "# ------\n",
    "# Best params for f1: {'n_neighbors': np.int64(3), 'weights': 'uniform'}\n",
    "# Best CV f1: 0.45962787039846775\n",
    "# ------\n",
    "# Best params for precision: {'n_neighbors': np.int64(25), 'weights': 'distance'}\n",
    "# Best CV precision: 0.8350000000000002\n",
    "# ------\n",
    "# Best params for recall: {'n_neighbors': np.int64(2), 'weights': 'distance'}\n",
    "# Best CV recall: 0.39583333333333337\n",
    "# ------\n",
    "# Best params for roc_auc: {'n_neighbors': np.int64(24), 'weights': 'distance'}\n",
    "# Best CV roc_auc: 0.75859375\n",
    "# ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_knn = grid_knn.best_estimator_\n",
    "\n",
    "y_pred = best_model_knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cfc32",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88b3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "# thanks to: https://medium.com/@kalpit.sharma/mastering-random-forest-hyperparameter-tuning-for-enhanced-machine-learning-models-2d1a8c6c426f\n",
    "\n",
    "# no need to scale numerical features\n",
    "preprocess_rf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    (\"preprocess\", preprocess_rf),\n",
    "    (\"model\", RandomForestClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "# notice that n=1000 and sqrt(1000) ~= 31, there are 20 features.\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [50, 100, 150],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__max_depth': [None, 10, 20, 30],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=pipeline_rf,  \n",
    "    param_grid=param_grid_rf, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
    "# ran for 11m 49.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0133ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all metrics, print best parameters\n",
    "cv_results_rf = grid_rf.cv_results_\n",
    "print_best_params_by_metric(cv_results_rf, metrics)\n",
    "\n",
    "# Best params for accuracy: {'criterion': 'entropy', 'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': np.int64(1), 'min_samples_split': np.int64(10), 'n_estimators': np.int64(100)}\n",
    "# Best CV accuracy: 0.7775\n",
    "# ------\n",
    "# Best params for f1: {'criterion': 'entropy', 'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': np.int64(1), 'min_samples_split': np.int64(10), 'n_estimators': np.int64(100)}\n",
    "# Best CV f1: 0.5242081346803799\n",
    "# ------\n",
    "# Best params for precision: {'criterion': 'entropy', 'max_depth': 30, 'max_features': 'log2', 'min_samples_leaf': np.int64(4), 'min_samples_split': np.int64(5), 'n_estimators': np.int64(50)}\n",
    "# Best CV precision: 0.8228588405058993\n",
    "# ------\n",
    "# Best params for recall: {'criterion': 'entropy', 'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': np.int64(1), 'min_samples_split': np.int64(10), 'n_estimators': np.int64(100)}\n",
    "# Best CV recall: 0.4125\n",
    "# ------\n",
    "# Best params for roc_auc: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': np.int64(1), 'min_samples_split': np.int64(5), 'n_estimators': np.int64(50)}\n",
    "# Best CV roc_auc: 0.8017113095238095\n",
    "# ------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a39c9c5",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Docs:\n",
    "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a724ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "# thanks to: https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning\n",
    "#       https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html\n",
    "#       https://xgboost.readthedocs.io/en/stable/parameter.html \n",
    "#       https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/#h-learning-task-parameters\n",
    "\n",
    "# no need to scale numerical features\n",
    "preprocess_xgb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_xgb = Pipeline([\n",
    "    (\"preprocess\", preprocess_xgb),\n",
    "    (\"model\", XGBClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'model__n_estimators': [50, 100, 150],\n",
    "    'model__gamma': [0, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__min_child_weight': [1, 3, 5],\n",
    "    'model__subsample': [0.7, 0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    estimator=pipeline_xgb,  \n",
    "    param_grid=param_grid_xgb, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
    "# ran for 6m 6.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all metrics, print best parameters\n",
    "cv_results_xgb = grid_xgb.cv_results_\n",
    "print_best_params_by_metric(cv_results_xgb, metrics)\n",
    "\n",
    "# Best params for accuracy: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.1), 'max_depth': np.int64(3), 'min_child_weight': np.int64(1), 'n_estimators': np.int64(50), 'subsample': np.float64(0.7)}\n",
    "# Best CV accuracy: 0.765\n",
    "# ------\n",
    "# Best params for f1: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.1), 'max_depth': np.int64(3), 'min_child_weight': np.int64(1), 'n_estimators': np.int64(50), 'subsample': np.float64(0.7)}\n",
    "# Best CV f1: 0.5708481307518998\n",
    "# ------\n",
    "# Best params for precision: {'colsample_bytree': np.float64(0.7), 'gamma': np.float64(0.0), 'max_depth': np.int64(7), 'min_child_weight': np.int64(1), 'n_estimators': np.int64(150), 'subsample': np.float64(0.7)}\n",
    "# Best CV precision: 0.6354906231094979\n",
    "# ------\n",
    "# Best params for recall: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.1), 'max_depth': np.int64(7), 'min_child_weight': np.int64(3), 'n_estimators': np.int64(50), 'subsample': np.float64(0.8)}\n",
    "# Best CV recall: 0.5250000000000001\n",
    "# ------\n",
    "# Best params for roc_auc: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.2), 'max_depth': np.int64(3), 'min_child_weight': np.int64(1), 'n_estimators': np.int64(50), 'subsample': np.float64(0.7)}\n",
    "# Best CV roc_auc: 0.79140625\n",
    "# ------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc33e0e",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "models = {\n",
    "    \"logistic_regression\": grid_lr,\n",
    "    \"knn\": grid_knn,\n",
    "    \"random_forest\": grid_rf,\n",
    "    \"xgboost\": grid_xgb\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    path = f\"models/{name}.pkl\"\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"Saved {name} to {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
