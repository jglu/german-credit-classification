{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 04 - Evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "SEED=42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/processed/german.csv\")\n",
    "\n",
    "# map credit_risk to 0/1 (good/bad)\n",
    "y = df[\"credit_risk\"].map({1: 0, 2: 1})\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "X = df.drop(columns=[\"credit_risk\", \"id\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## output best params found in hyperparameter tuning\n",
    "def print_best_params_by_metric(cv_results, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Outputs best parameter set for each metric.\n",
    "    \"\"\"\n",
    "    \n",
    "    # using same scoring metrics across all four models\n",
    "    metrics = [\"accuracy\", \"f1\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "    \n",
    "    print(f\"For {model_name}:\")\n",
    "    for metric in metrics:\n",
    "        best_index = cv_results[f\"mean_test_{metric}\"].argmax()\n",
    "        best_params = {k.replace(\"param_model__\", \"\"): v[best_index] \n",
    "                       for k, v in cv_results.items()\n",
    "                        if k.startswith(\"param_model__\")}\n",
    "        best_score = cv_results[f\"mean_test_{metric}\"][best_index]\n",
    "        print(f\"Best params for {metric}: {best_params}\")\n",
    "        print(f\"Best CV {metric}: {best_score}\")\n",
    "    print(\"------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## output test metrics for best fit model\n",
    "def print_test_metrics(model, X_test, y_test, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Prints test set metrics for a fitted model in a simple key-value style.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: fitted sklearn pipeline or classifier\n",
    "    - model_name: string for labeling output\n",
    "    \"\"\"\n",
    "    best_params = model.best_params_\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_prob)\n",
    "    }\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"Test {metric}: {value}\")\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import saved models\n",
    "loaded_models = {}\n",
    "for name in [\"logistic_regression\", \"knn\", \"random_forest\", \"xgboost\"]:\n",
    "    path = f\"saved_models/{name}.pkl\"\n",
    "    loaded_models[name] = joblib.load(path)\n",
    "\n",
    "lr_model = loaded_models[\"logistic_regression\"]\n",
    "knn_model = loaded_models[\"knn\"]\n",
    "rf_model = loaded_models[\"random_forest\"]\n",
    "xgb_model = loaded_models[\"xgboost\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each model, output best parameters\n",
    "print_best_params_by_metric(lr_model.cv_results_, \"Logistic Regression\")\n",
    "print_best_params_by_metric(knn_model.cv_results_,\"kNN\")\n",
    "print_best_params_by_metric(rf_model.cv_results_, \"Random Forest\")\n",
    "print_best_params_by_metric(xgb_model.cv_results_, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Logistic Regression:\n",
    "# Best params for accuracy: {'C': np.float64(0.615848211066026), 'max_iter': np.int64(100), 'penalty': 'l2', 'solver': 'lbfgs', 'l1_ratio': masked}\n",
    "# Best CV accuracy: 0.75125\n",
    "# Best params for f1: {'C': np.float64(4.281332398719396), 'max_iter': np.int64(1000), 'penalty': 'elasticnet', 'solver': 'saga', 'l1_ratio': np.float64(0.75)}\n",
    "# Best CV f1: 0.5278343535463698\n",
    "# Best params for precision: {'C': np.float64(0.004832930238571752), 'max_iter': np.int64(100), 'penalty': 'l2', 'solver': 'liblinear', 'l1_ratio': masked}\n",
    "# Best CV precision: 0.7171428571428571\n",
    "# Best params for recall: {'C': np.float64(78.47599703514607), 'max_iter': np.int64(100), 'penalty': 'l2', 'solver': 'newton-cg', 'l1_ratio': masked}\n",
    "# Best CV recall: 0.4791666666666667\n",
    "# Best params for roc_auc: {'C': np.float64(0.615848211066026), 'max_iter': np.int64(100), 'penalty': 'l2', 'solver': 'sag', 'l1_ratio': masked}\n",
    "# Best CV roc_auc: 0.7853050595238096\n",
    "# ------\n",
    "\n",
    "# For kNN:\n",
    "# Best params for accuracy: {'n_neighbors': np.int64(10), 'weights': 'distance'}\n",
    "# Best CV accuracy: 0.7425\n",
    "# Best params for f1: {'n_neighbors': np.int64(3), 'weights': 'uniform'}\n",
    "# Best CV f1: 0.45962787039846775\n",
    "# Best params for precision: {'n_neighbors': np.int64(25), 'weights': 'distance'}\n",
    "# Best CV precision: 0.8350000000000002\n",
    "# Best params for recall: {'n_neighbors': np.int64(2), 'weights': 'distance'}\n",
    "# Best CV recall: 0.39583333333333337\n",
    "# Best params for roc_auc: {'n_neighbors': np.int64(24), 'weights': 'distance'}\n",
    "# Best CV roc_auc: 0.75859375\n",
    "# ------\n",
    "\n",
    "# For Random Forest:\n",
    "# Best params for accuracy: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': np.int64(1), 'min_samples_split': np.int64(5), 'n_estimators': np.int64(50)}\n",
    "# Best CV accuracy: 0.7775000000000001\n",
    "# Best params for f1: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': np.int64(1), 'min_samples_split': np.int64(5), 'n_estimators': np.int64(50)}\n",
    "# Best CV f1: 0.5223626341081272\n",
    "# Best params for precision: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': np.int64(1), 'min_samples_split': np.int64(5), 'n_estimators': np.int64(100)}\n",
    "# Best CV precision: 0.8001082251082252\n",
    "# Best params for recall: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': np.int64(1), 'min_samples_split': np.int64(5), 'n_estimators': np.int64(50)}\n",
    "# Best CV recall: 0.4083333333333334\n",
    "# Best params for roc_auc: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': np.int64(2), 'min_samples_split': np.int64(10), 'n_estimators': np.int64(150)}\n",
    "# Best CV roc_auc: 0.7994419642857143\n",
    "# ------\n",
    "\n",
    "# For XGBoost:\n",
    "# Best params for accuracy: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.2), 'max_depth': np.int64(7), 'min_child_weight': np.int64(3), 'n_estimators': np.int64(50), 'subsample': np.float64(0.7)}\n",
    "# Best CV accuracy: 0.7700000000000001\n",
    "# Best params for f1: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.2), 'max_depth': np.int64(7), 'min_child_weight': np.int64(3), 'n_estimators': np.int64(50), 'subsample': np.float64(0.7)}\n",
    "# Best CV f1: 0.573550385273438\n",
    "# Best params for precision: {'colsample_bytree': np.float64(0.7), 'gamma': np.float64(0.1), 'max_depth': np.int64(3), 'min_child_weight': np.int64(3), 'n_estimators': np.int64(50), 'subsample': np.float64(1.0)}\n",
    "# Best CV precision: 0.6433135949248482\n",
    "# Best params for recall: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.2), 'max_depth': np.int64(5), 'min_child_weight': np.int64(3), 'n_estimators': np.int64(100), 'subsample': np.float64(0.8)}\n",
    "# Best CV recall: 0.5333333333333333\n",
    "# Best params for roc_auc: {'colsample_bytree': np.float64(0.8), 'gamma': np.float64(0.1), 'max_depth': np.int64(3), 'min_child_weight': np.int64(1), 'n_estimators': np.int64(50), 'subsample': np.float64(0.7)}\n",
    "# Best CV roc_auc: 0.7905133928571428\n",
    "# ------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each best model, output test metrics\n",
    "print_test_metrics(lr_model, X_test, y_test, \"Logistic Regression\")\n",
    "print_test_metrics(knn_model, X_test, y_test, \"kNN\")\n",
    "print_test_metrics(rf_model, X_test, y_test, \"Random Forest\")\n",
    "print_test_metrics(xgb_model, X_test, y_test, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters for Logistic Regression: {'model__C': np.float64(0.615848211066026), 'model__max_iter': 100, 'model__penalty': 'l2', 'model__solver': 'lbfgs'}\n",
    "# Test accuracy: 0.785\n",
    "# Test f1: 0.5904761904761905\n",
    "# Test precision: 0.6888888888888889\n",
    "# Test recall: 0.5166666666666667\n",
    "# Test roc_auc: 0.8051190476190476\n",
    "# ------\n",
    "# Best parameters for kNN: {'model__n_neighbors': np.int64(10), 'model__weights': 'distance'}\n",
    "# Test accuracy: 0.75\n",
    "# Test f1: 0.46808510638297873\n",
    "# Test precision: 0.6470588235294118\n",
    "# Test recall: 0.36666666666666664\n",
    "# Test roc_auc: 0.7516666666666666\n",
    "# ------\n",
    "# Best parameters for Random Forest: {'model__criterion': 'entropy', 'model__max_depth': 20, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 5, 'model__n_estimators': 50}\n",
    "# Test accuracy: 0.76\n",
    "# Test f1: 0.48936170212765956\n",
    "# Test precision: 0.6764705882352942\n",
    "# Test recall: 0.38333333333333336\n",
    "# Test roc_auc: 0.7735714285714286\n",
    "# ------\n",
    "# Best parameters for XGBoost: {'model__colsample_bytree': 0.8, 'model__gamma': 0.2, 'model__max_depth': 7, 'model__min_child_weight': 3, 'model__n_estimators': 50, 'model__subsample': 0.7}\n",
    "# Test accuracy: 0.755\n",
    "# Test f1: 0.5333333333333333\n",
    "# Test precision: 0.6222222222222222\n",
    "# Test recall: 0.4666666666666667\n",
    "# Test roc_auc: 0.7428571428571428\n",
    "# ------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
