{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 03 - Models\n",
    "\n",
    "Models:\n",
    "- Logistic regression\n",
    "- kNN\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- Categorical Naive Bayes\n",
    "- SVM\n",
    "- Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"./data/processed/german.csv\")\n",
    "\n",
    "y = df[\"credit_risk\"]\n",
    "X = df.drop(columns=[\"credit_risk\", \"id\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Shared code for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used in transforming the data with ColumnTransformer()\n",
    "numeric_features = X.select_dtypes(include=\"number\").columns\n",
    "categorical_features = X.select_dtypes(exclude=\"number\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## output best params found in hyperparameter tuning\n",
    "\n",
    "# using same scoring metrics across all four models\n",
    "metrics = [\"accuracy\", \"f1\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"f1\": make_scorer(f1_score, zero_division=0),\n",
    "    \"precision\": make_scorer(precision_score, zero_division=0),\n",
    "    \"recall\": make_scorer(recall_score, zero_division=0),\n",
    "    \"roc_auc\": \"roc_auc\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "np.logspace(-4,4,20) = (array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n",
    "        4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
    "        2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
    "        1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
    "        5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "# thanks to: https://www.youtube.com/watch?v=tIO8zPCdi58\n",
    "# thanks to: https://www.geeksforgeeks.org/machine-learning/how-to-optimize-logistic-regression-performance/\n",
    "\n",
    "preprocess_lr = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    (\"preprocess\", preprocess_lr),\n",
    "    (\"model\", LogisticRegression(random_state=SEED))\n",
    "])\n",
    "\n",
    "# \"The main hyperparameters we may tune in logistic regression are: solver, penalty, and regularization strength (sklearn documentation).\"\n",
    "#   from: https://medium.com/codex/do-i-need-to-tune-logistic-regression-hyperparameters-1cb2b81fca69\n",
    "# \"Logistic regression does not really have any critical hyperparameters to tune.\"\n",
    "#   from: https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "\n",
    "param_grid_lr = [\n",
    "    {'model__penalty': ['l1'], 'model__solver': ['liblinear', 'saga'], 'model__C': np.logspace(-4, 4, 20), \"model__max_iter\": [100,1000,2500,5000]},\n",
    "    {'model__penalty': ['l2'], 'model__solver': ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga'], 'model__C': np.logspace(-4, 4, 20), \"model__max_iter\": [100,1000,2500,5000]},\n",
    "    {'model__penalty': ['elasticnet'], 'model__solver': ['saga'], 'model__C': np.logspace(-4, 4, 20), 'model__l1_ratio': [0.25, 0.5, 0.75], \"model__max_iter\": [100,1000,2500,5000]}\n",
    "]\n",
    "\n",
    "grid_lr = GridSearchCV(\n",
    "    estimator=pipeline_lr,  \n",
    "    param_grid=param_grid_lr, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "# Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n",
    "# ran for like 4m..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "# thanks to: https://medium.com/@agrawalsam1997/hyperparameter-tuning-of-knn-classifier-a32f31af25c7\n",
    "#   for this: np.arange(2, 30, 1)\n",
    "\n",
    "# i purposely don't drop=\"first\"\n",
    "preprocess_knn = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_knn = Pipeline([\n",
    "    (\"preprocess\", preprocess_knn),\n",
    "    (\"model\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# notice that n=1000 and sqrt(1000) ~= 31\n",
    "param_grid_knn = {\n",
    "    'model__n_neighbors': np.arange(2, 30, 1), \n",
    "    \"model__weights\": [\"uniform\", \"distance\"],\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(\n",
    "    estimator=pipeline_knn,  \n",
    "    param_grid=param_grid_knn, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "# ran for 14.1s very fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "# thanks to: https://medium.com/@kalpit.sharma/mastering-random-forest-hyperparameter-tuning-for-enhanced-machine-learning-models-2d1a8c6c426f\n",
    "\n",
    "# no need to scale numerical features\n",
    "preprocess_rf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    (\"preprocess\", preprocess_rf),\n",
    "    (\"model\", RandomForestClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "# notice that n=1000 and sqrt(1000) ~= 31, there are 20 features.\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [50, 100, 150],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__max_depth': [None, 10, 20, 30],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=pipeline_rf,  \n",
    "    param_grid=param_grid_rf, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
    "# ran for 11m 49.6s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Docs:\n",
    "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "# thanks to: https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning\n",
    "#       https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html\n",
    "#       https://xgboost.readthedocs.io/en/stable/parameter.html \n",
    "#       https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/#h-learning-task-parameters\n",
    "\n",
    "# no need to scale numerical features\n",
    "preprocess_xgb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_xgb = Pipeline([\n",
    "    (\"preprocess\", preprocess_xgb),\n",
    "    (\"model\", XGBClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'model__n_estimators': [50, 100, 150],\n",
    "    'model__gamma': [0, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__min_child_weight': [1, 3, 5],\n",
    "    'model__subsample': [0.7, 0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    estimator=pipeline_xgb,  \n",
    "    param_grid=param_grid_xgb, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
    "# ran for 6m 6.5s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Our data is a mix of both categorical and numerical so we cannot correctly out of the box use GaussianNB or CategoricalNB. Our dataset has more categorical covariates than numerical (13 > 7), so we choose to use CategoricalNB with encoding.\n",
    "\n",
    "- with help from https://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin numeric features into discrete categories\n",
    "# use OrdinalEncoder() for categorical bc CategoricalNB() doesn't consider order\n",
    "preprocess_nb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile'), numeric_features),\n",
    "        ('cat', OrdinalEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_nb = Pipeline([\n",
    "    ('preprocess', preprocess_nb),\n",
    "    ('model', CategoricalNB())\n",
    "])\n",
    "\n",
    "param_grid_nb = {\n",
    "    'model__alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "    'model__min_categories': [0, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "grid_nb = GridSearchCV(\n",
    "    estimator=pipeline_nb,  \n",
    "    param_grid=param_grid_nb, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_svm = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_svm = Pipeline([\n",
    "    ('preprocess', preprocess_svm),\n",
    "    ('model', SVC(random_state=SEED))\n",
    "])\n",
    "\n",
    "param_grid_svm = {\n",
    "    'model__C': [0.01, 0.1, 1, 10],\n",
    "    'model__degree': [2, 3, 4, 5],\n",
    "    'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'model__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_svm = GridSearchCV(\n",
    "    estimator=pipeline_svm,  \n",
    "    param_grid=param_grid_svm, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_nn = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_nn = Pipeline([\n",
    "    ('preprocess', preprocess_nn),\n",
    "    ('model', MLPClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "param_grid_nn = {\n",
    "    'model__hidden_layer_sizes': [(50,), (100,), (50,50), (100,50), (100,100)],\n",
    "    'model__activation': ['relu', 'tanh', 'logistic'],\n",
    "    'model__solver': ['adam', 'sgd'],\n",
    "    'model__alpha': [0.0001, 0.001, 0.01, 0.1],       # L2 regularization\n",
    "    'model__learning_rate': ['constant', 'adaptive'],\n",
    "    'model__learning_rate_init': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "grid_nn = GridSearchCV(\n",
    "    estimator=pipeline_nn,  \n",
    "    param_grid=param_grid_nn, \n",
    "    cv=5,                    \n",
    "    scoring=scoring,\n",
    "    refit=\"accuracy\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "models = {\n",
    "    \"logistic_regression\": grid_lr,\n",
    "    \"knn\": grid_knn,\n",
    "    \"random_forest\": grid_rf,\n",
    "    \"xgboost\": grid_xgb,\n",
    "    \"naive_bayes\": grid_nb,\n",
    "    \"svm\": grid_svm,\n",
    "    \"neural_network\": grid_nn\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    path = f\"saved_models/{name}.pkl\"\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"Saved {name} to {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
